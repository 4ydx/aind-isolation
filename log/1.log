
This script evaluates the performance of the custom heuristic function by
comparing the strength of an agent using iterative deepening (ID) search with
alpha-beta pruning against the strength rating of agents using other heuristic
functions.  The `ID_Improved` agent provides a baseline by measuring the
performance of a basic agent using Iterative Deepening and the "improved"
heuristic (from lecture) on your hardware.  The `Student` agent then measures
the performance of Iterative Deepening and the custom heuristic against the
same opponents.


*************************
 Evaluating: ID_Improved 
*************************

Playing Matches:
----------
  Match 1: ID_Improved vs   Random    	Result: 187 to 13
  Match 2: ID_Improved vs   MM_Null   	Result: 169 to 31
  Match 3: ID_Improved vs   MM_Open   	Result: 158 to 42
  Match 4: ID_Improved vs MM_Improved 	Result: 144 to 56
  Match 5: ID_Improved vs   AB_Null   	Result: 148 to 52
  Match 6: ID_Improved vs   AB_Open   	Result: 139 to 61
  Match 7: ID_Improved vs AB_Improved 	Result: 125 to 75


Results:
----------
ID_Improved         76.43%

*************************
   Evaluating: Student   
*************************

Playing Matches:
----------
  Match 1:   Student   vs   Random    	Result: 185 to 15
  Match 2:   Student   vs   MM_Null   	Result: 174 to 26
  Match 3:   Student   vs   MM_Open   	Result: 140 to 60
  Match 4:   Student   vs MM_Improved 	Result: 136 to 64
  Match 5:   Student   vs   AB_Null   	Result: 157 to 43
  Match 6:   Student   vs   AB_Open   	Result: 124 to 76
  Match 7:   Student   vs AB_Improved 	Result: 111 to 89


Results:
----------
Student             73.36%
